[![Live Demo](https://img.shields.io/badge/Live-Demo-success?logo=vercel)](https://tanveer-docuchat-ai.vercel.app)
# ğŸ“„ AI PDF RAG â€“ Streaming Chat with Sources
DocuChat AI is a production-ready, **Retrieval-Augmented Generation (RAG)** powered PDF chat application that allows users to upload documents and ask natural language questions â€” with cited page references and transparent source tracking. built with:

![Chat Screen or OpenGraph Image](./public/opengraph-image.png)

* **Next.js (App Router)**
* **Streaming LLM responses**
* **OpenRouter**
* **Supabase + pgvector**
* **HuggingFace embeddings**

Upload a PDF â†’ Generate embeddings â†’ Store vectors â†’ Ask questions â†’ Receive **streamed answers with sources**.

---

# ğŸš€ Features

* ğŸ“¥ PDF upload & validation
* âœ‚ï¸ Character-aware chunking
* ğŸ“„ Page-aware source mapping
* ğŸ§  Batched embedding generation
* ğŸ—„ Vector storage using pgvector
* ğŸ” Semantic similarity search
* ğŸ’¬ Streaming AI responses
* ğŸ“š Source injection before answer
* ğŸ›¡ Safe error handling & stream cleanup

---

# ğŸ— Architecture Overview

```text
Client
   â†“
/api/process-pdf
   â†“
Validation â†’ Extraction â†’ Chunking â†’ Embeddings â†’ Supabase (pgvector)

Client
   â†“
/api/chat
   â†“
Query Embedding â†’ Vector Search â†’ System Prompt Builder
   â†“
OpenRouter LLM (streamed)
   â†“
ReadableStream â†’ Client
```

---

# ğŸ“‚ Important Folder Structure

## App Routes

```
app/
 â””â”€â”€ api/
     â”œâ”€â”€ chat/route.ts          # Streaming RAG chat endpoint
     â””â”€â”€ process-pdf/route.ts   # PDF processing & embedding pipeline
```

---

## Core RAG Logic

```
lib/
 â”œâ”€â”€ pdf/
 â”‚   â”œâ”€â”€ validation.ts        # File + content validation
 â”‚   â”œâ”€â”€ extractText.ts       # PDF text extraction
 â”‚   â”œâ”€â”€ chunkText.ts         # Text chunking (char-aware)
 â”‚   â”œâ”€â”€ pageMapping.ts       # Page approximation logic
 â”‚   â”œâ”€â”€ createEmbedding.ts   # HuggingFace embedding generation
 â”‚   â”œâ”€â”€ getEmbedding.ts      # Single query embedding
 â”‚   â””â”€â”€ saveEmbeddings.ts    # Batched vector insert
 â”‚
 â”œâ”€â”€ buildSystemPrompt.ts     # Injects sources into prompt
 â”œâ”€â”€ supabaseAdmin.ts         # Supabase admin client
 â”œâ”€â”€ tokenizer.ts             # Token counting utilities
 â”œâ”€â”€ utils.ts                 # Shared helpers
 â””â”€â”€ withTimeoutPromise.ts    # Timeout protection wrapper
```

---

## UI Components

```
components/
 â”œâ”€â”€ ChatInterface.tsx
 â”œâ”€â”€ FileUpload.tsx
 â”œâ”€â”€ RoleSelector.tsx
 â””â”€â”€ ui/                      # Reusable UI primitives
```

---

# ğŸ“¡ API Endpoints

## 1ï¸âƒ£ Process PDF

```
POST /api/process-pdf
```

* Validates file
* Extracts text
* Chunks document
* Generates embeddings
* Stores vectors in Supabase

Response:

```json
{
  "success": true,
  "sessionId": "uuid"
}
```

---

## 2ï¸âƒ£ Chat with Document

```
POST /api/chat
```

* Embeds user question
* Performs vector similarity search
* Builds system prompt with sources
* Streams LLM response

Response:

* Streaming text output
* Sources metadata sent first
* Clean stream termination

---

# ğŸ”„ Streaming Strategy

The `/api/chat` endpoint:

1. Sends sources immediately
2. Streams model tokens progressively
3. Handles cancellation safely
4. Cleans up reader on failure
5. Injects fallback `[STREAM_ERROR]` marker if needed

This ensures:

* Low latency
* Minimal memory usage
* Production-safe streaming behavior

---

# ğŸ§  Retrieval Flow

1. User question â†’ embedding
2. Vector similarity search (pgvector)
3. Top chunks retrieved
4. System prompt constructed with citations
5. LLM generates streamed answer

---

# ğŸ” Environment Variables

```
OPENROUTER_API_KEY=
HUGGINGFACE_API_KEY=
SUPABASE_URL=
SUPABASE_SERVICE_ROLE_KEY=
```

---

# âš™ï¸ Tech Stack

* Next.js 14+
* TypeScript
* Supabase
* pgvector
* OpenRouter
* HuggingFace
* LangChain TextSplitter

---

# ğŸ›¡ Production Considerations

* Batched embedding inserts
* Chunk limits enforced
* Validation before processing
* Stream cancellation handling
* Timeout protection utilities
* Modular architecture for scaling

---

# ğŸ“ˆ Future Enhancements

* Background job queue for PDF processing
* Hybrid search (BM25 + vector)
* Reranking layer
* Redis caching
* Multi-tenant isolation
* Rate limiting
* Document status tracking

---

# ğŸ“œ License

MIT

---
## ğŸ‘¨â€ğŸ’» About the Author

Built with a focus on scalable RAG architecture, streaming LLM integration, and production-ready design.

**Tanveer H.**  
Frontend AI Developer  

ğŸŒ Portfolio: https://tanveer-portfolio.vercel.app/en-US/work
ğŸ’¼ LinkedIn: https://linkedin.com/in/tanveer-h1  
ğŸ“§ Contact: https://tanveer-portfolio.vercel.app/en-US/contact  
